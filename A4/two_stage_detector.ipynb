{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# EECS 498-007/598-005 Assignment 5-2: Two-Stage Object Detector - Faster R-CNN\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "JSXasOiouZdl",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Answer:**   \n",
    "Firstname Lastname, #UMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "PoRTyUc94S1a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Faster R-CNN: A Classic Two-Stage Anchor-Based Object Detector\n",
    "\n",
    "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules - Region Proposal Networks (RPN) and Fast R-CNN.\n",
    "Like one-stage detector in the first part of our assignment,\n",
    "we will train it to detect a set of object classes and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "LfBk3NtRgqaV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "ubB_0e-UAOVK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1604336756706,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "ASkY27ZtA7Is",
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "MzqbYcKdz6ew",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1604336756898,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "HzRdJ3uhe1CR",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "fa9a4e6e-6da7-426c-cdfc-7cf69b12e524",
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "OvUDZWGU3VLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"common.py\", \"one_stage_detector.ipynb\", \"two_stage_detector_faster_rcnn.ipynb\", \"eecs598\", \"one_stage_detector.py\", \"two_stage_detector.py\", \"a4_helper.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1604336756898,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "RrAX9FOLpr9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "439f3a5e-a10f-4c84-ba41-97e9718cdde3",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2022WI folder and put all the files under A4 folder, then \"2022WI/A4\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "RldDumJE48pv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from common.py!\n",
    "Hello from two_stage_detector.py!\n",
    "Hello from a4_helper.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `two_stage_detector.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1604336757413,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "pTIwSpkS495_",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "bd251f4e-5639-412b-93ac-f0ddce7c0dc0",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from common import hello_common\n",
    "from two_stage_detector import hello_two_stage_detector\n",
    "from a4_helper import hello_helper\n",
    "\n",
    "\n",
    "hello_common()\n",
    "hello_two_stage_detector()\n",
    "hello_helper()\n",
    "\n",
    "two_stage_detector_path = os.path.join(GOOGLE_DRIVE_PATH, \"two_stage_detector.py\")\n",
    "two_stage_detector_edit_time = time.ctime(\n",
    "    os.path.getmtime(two_stage_detector_path)\n",
    ")\n",
    "print(\"two_stage_detector.py last edited on %s\" % two_stage_detector_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "GWP1vCGL5Eca",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 5651,
     "status": "ok",
     "timestamp": 1604336761995,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "CwVZ26yM5G8U",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "a76987a8-55fc-438f-8859-a23aed779c54",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from a4_helper import *\n",
    "from eecs598 import reset_seed\n",
    "from eecs598.grad import rel_error\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "# for mAP evaluation\n",
    "!rm -rf mAP\n",
    "!git clone https://github.com/Cartucho/mAP.git\n",
    "!rm -rf mAP/input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "x7poKGI35JZY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 5644,
     "status": "ok",
     "timestamp": 1604336761999,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "Vw3wIuCu5LnU",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "0c522ba2-0912-4e74-b895-7b491a4eddbf",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "MjJ3uyYBg3Lw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load PASCAL VOC 2007 data\n",
    "As in the previous notebook, we will use PASCAL VOC 2007 dataset to train our model. The following two cells are exactly same as those in `one_stage_detector.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 32177,
     "status": "ok",
     "timestamp": 1604336788554,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "MmEP5KQJzk0d",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "1f13cb78-175c-40c0-d6b3-ea22bcf990de",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment below to use the mirror link if the original link is broken and move it to GOOGLE_DRIVE_PATH\n",
    "# !wget https://web.eecs.umich.edu/~justincj/data/VOCtrainval_06-Nov-2007.tar\n",
    "# import shutil\n",
    "# shutil.move(\"VOCtrainval_06-Nov-2007.tar\", os.path.join(GOOGLE_DRIVE_PATH, \"VOCtrainval_06-Nov-2007.tar\"))\n",
    "from a4_helper import VOC2007DetectionTiny\n",
    "\n",
    "\n",
    "# NOTE: Set `download=True` for the first time when you set up Google Drive folder.\n",
    "# It will automatically download VOC 2007 dataset. Turn it back to `False` later\n",
    "# for faster execution in the future.\n",
    "train_dataset = VOC2007DetectionTiny(\n",
    "    GOOGLE_DRIVE_PATH, \"train\", image_size=IMAGE_SHAPE[0],\n",
    "    download=False  # True (for the first time)\n",
    ")\n",
    "val_dataset = VOC2007DetectionTiny(\n",
    "    GOOGLE_DRIVE_PATH, \"val\", image_size=IMAGE_SHAPE[0],\n",
    "    download=False  # True (for the first time)\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap these dataset objects with PyTorch dataloaders, similar to `one_stage_detector.ipynb`. The format of output batches will also be same as what you have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
    "# on the main CPU process, suitable for Colab.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Use batch_size = 1 during inference - during inference we do not center crop\n",
    "# the image to detect all objects, hence they may be of different size. It is\n",
    "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "image_paths, images, gt_boxes = train_loader_iter.next()\n",
    "\n",
    "print(f\"image paths           : {image_paths}\")\n",
    "print(f\"image batch has shape : {images.shape}\")\n",
    "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
    "\n",
    "print(f\"Five boxes per image  :\")\n",
    "print(gt_boxes[:, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "X4WmocEyiXWa",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Visualize PASCAL VOC 2007\n",
    "\n",
    "We will visualize a few images and their GT boxes, just to make sure that everything is loaded properly. You would have already seen these visualizations (and the code snippet below) in `one_stage_detector.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 36626,
     "status": "ok",
     "timestamp": 1604336793027,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "ld1s28Z4fyL5",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "b2c9a1a3-e4ea-4d15-8a05-22c45f80323d",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from eecs598.utils import detection_visualizer\n",
    "\n",
    "inverse_norm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
    "    if idx > 2:\n",
    "        break\n",
    "\n",
    "    image = inverse_norm(image)\n",
    "    is_valid = gt_boxes[:, 4] >= 0\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone with Feature Pyramid Networks (FPN)\n",
    "\n",
    "Faster R-CNN uses a convolutional backbone with FPN in the exact same way as you implemented in FCOS. So you can directly re-use it for this part of the assignment.\n",
    "\n",
    "**NOTE:** Typical state-of-the-art detectors based o nFaster R-CNN use four multi-scale features from different FPN levels — `(p2, p3, p4, p5)` with strides `(4, 8, 16, 32)`.\n",
    "Due to computational limits of Google Colab, we will instead simply use `(p3, p4, p5)` features like FCOS.\n",
    "In all your implementations for this part, you may assume that you will receive features from these three FPN levels (and may hard-code these names as Python strings). Your code will not be tested with `p2` FPN features and you will not lose points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from common import DetectorBackboneWithFPN\n",
    "from two_stage_detector import RPNPredictionNetwork\n",
    "\n",
    "\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
    "\n",
    "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
    "dummy_images = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Collect dummy output.\n",
    "dummy_fpn_feats = backbone(dummy_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "oRc7P-RvRZGZ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Faster R-CNN first stage: Region Proposal Network (RPN)\n",
    "\n",
    "We will now implement the first-stage of Faster R-CNN. It comprises a **Region Proposal Network (RPN)** that learns to predict general _object proposals_, which will then be used by the second stage to make final predictions.\n",
    "\n",
    "**RPN prediction:** An input image is passed through the backbone and we obtain its FPN feature maps `(p3, p4, p5)`.\n",
    "The RPN predicts multiple values at _every location on FPN features_. Faster R-CNN is _anchor-based_ — the model assumes that every location has multiple pre-defined boxes (called \"anchors\") and it predicts two measures per anchor, per FPN location:\n",
    "\n",
    "1. **Objectness:** The likelihood of having _any_ object inside the anchor. This is similar to classification head in FCOS, except that this is _class-agnostic_: it only performs binary foreground/background classification.\n",
    "2. **Box regression deltas:** 4-D \"deltas\" that _transform_ an anchor at that location to a ground-truth box.\n",
    "\n",
    "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
    "\n",
    "**SIDE NOTE:** We will use the more common practice of predicting `k` logits and use a logistic regressor instead of `2k` scores (and 2-way softmax) as shown in Figure. This slightly reduces the number of trainable parameters.\n",
    "\n",
    "This RPN is conceptually quite similar to a one-stage detector like FCOS.\n",
    "The main differences with what you implemented in FCOS are: (1) RPN is anchor-based, and make predictions for multiple anchor boxes instead of location \"points\", (2) it performs class-agnostic object classification, and (3) it excludes centerness regression, which was inntroduced in FCOS itself, years after Faster R-CNN was published.\n",
    "\n",
    "Like we saw in FCOS, each anchor will be matched with a GT box for supervision — we will get to it shortly.\n",
    "For now, let's assume there are some `A` anchor boxes at every FPN location, and implement an RPN module.\n",
    "Structurally, this module is similar to FCOS prediction network.\n",
    "Now follow the instructions in `RPNPredictionNetwork` of `two_stage_detector.py` and implement layers to predict objectness and box regression deltas.\n",
    "Execute the following cell to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from two_stage_detector import RPNPredictionNetwork\n",
    "\n",
    "rpn_pred_net = RPNPredictionNetwork(\n",
    "    in_channels=64, stem_channels=[64], num_anchors=3\n",
    ")\n",
    "\n",
    "# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\n",
    "dummy_rpn_obj, dummy_rpn_box = rpn_pred_net(dummy_fpn_feats)\n",
    "\n",
    "# Few expected outputs:\n",
    "# Shape of p4 RPN objectness: torch.Size([2, 196, 3])\n",
    "# Shape of p5 RPN box deltas: torch.Size([2, 49, 12])\n",
    "\n",
    "print(f\"\\nFor dummy input images with shape: {dummy_images.shape}\")\n",
    "for level_name in dummy_fpn_feats.keys():\n",
    "    print(f\"Shape of {level_name} FPN features  : {dummy_fpn_feats[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN objectness: {dummy_rpn_obj[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN box deltas: {dummy_rpn_box[level_name].shape}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "etBYc7rbj35F",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Anchor-based Training of RPN\n",
    "\n",
    "Now that we implemented this RPN head, our goal is to have it predict _high objectness_ and _accurate box deltas_ for anchors that are likely to contain objects.\n",
    "Similar to first part of our assignment, we need to assign a target GT box to every RPN prediction for training supervision.\n",
    "\n",
    "**Recall FCOS location matching:** FCOS matched every FPN feature map location with a GT box (or marked them background), based on a heuristic that a location whether that location was _inside_ any GT Box.\n",
    "On the other hand, Faster R-CNN is anchor-based: instead of _locations_, it makes predictions with reference to some pre-defined _anchor boxes_, and matches each anchor with a single GT box if they have a high enough Intersection-over-Union (IoU).\n",
    "\n",
    "In the next few cells, we will perform the following steps, which are procedurally very similar to what you have already done with FCOS:\n",
    "\n",
    "1. **Anchor generation:** Generate a set of anchors for every location in FPN feature map.\n",
    "2. **Anchor to GT matching:** Match these anchors with GT boxes based on their IoU-overlap.\n",
    "3. **Format of box deltas:** Implement the tranformation functions to obtain _box deltas_ from GT boxes (model training supervision) and apply deltas to anchors (final proposal boxes for second stage).\n",
    "\n",
    "Let's approach these steps, one at a time.\n",
    "\n",
    "### Anchor Generation\n",
    "\n",
    "Recall that you already implemented a function to get the absolute image co-ordinates of FPN feature map locations, for FCOS — in `get_fpn_location_coords` of `common.py`.\n",
    "First we need to form multiple anchor boxes centered at these locations.\n",
    "RPN defines square anchor boxes of size `scale * stride` at every location, where `stride` is the FPN level stride, and `scale` is a hyperparameter.\n",
    "For example, anchor boxes for P5 level (`stride = 32`), with `scale = 2` will be boxes of `(64 x 64)` pixels.\n",
    "RPN also considers anchors of different aspect ratios, apart from square anchor boxes —\n",
    "follow the instructions in `generate_fpn_anchors` of `two_stage_detector.py` to implement creation of multiple anchor boxes per location.\n",
    "\n",
    "Execute the next cell to verify your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 37772,
     "status": "ok",
     "timestamp": 1604336794196,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "O5w-EUJekJj-",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "49162f9d-f073-4d9d-e9b8-541c6ae3f981",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from common import get_fpn_location_coords\n",
    "from two_stage_detector import generate_fpn_anchors\n",
    "\n",
    "\n",
    "# Sanity check: Get 2x2 location co-ordinates of p5 (original shape is 7x7).\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={\"p5\": (2, 64, 2, 2)}, strides_per_fpn_level={\"p5\": 32}\n",
    ")\n",
    "\n",
    "print(\"P5 locations:\\n\", locations[\"p5\"])\n",
    "\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={\"p5\": 32},\n",
    "    stride_scale=2,\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "print(\"P5 anchors with different aspect ratios:\")\n",
    "print(\"P5 1:2 anchors:\\n\", anchors[\"p5\"][0::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-29.2548,  -6.6274,  61.2548,  38.6274]\n",
    "# [-29.2548,  25.3726,  61.2548,  70.6274]\n",
    "# [  2.7452,  -6.6274,  93.2548,  38.6274]\n",
    "# [  2.7452,  25.3726,  93.2548,  70.6274]\n",
    "\n",
    "print(\"P5 1:1 anchors:\\n\", anchors[\"p5\"][1::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-16., -16.,  48.,  48.]\n",
    "# [-16.,  16.,  48.,  80.]\n",
    "# [ 16., -16.,  80.,  48.]\n",
    "# [ 16.,  16.,  80.,  80.]\n",
    "\n",
    "print(\"P5 2:1 anchors:\\n\", anchors[\"p5\"][2::3, :], \"\\n\")\n",
    "# Similar to 1:2 anchors, but with flipped co-ordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching anchor boxes with GT boxes\n",
    "\n",
    "Similar to FCOS, we will now match these generated anchors with GT boxes. Faster R-CNN matches some `N` anchor boxes with `M` GT boxes by applying two rules:\n",
    "\n",
    "> Anchor box $N_i$ is matched with box $M_i$ if it has an IoU overlap higher than 0.6 with that box. For multiple such GT boxes, the anchor is assigned with the GT box that has the highest IoU. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
    "\n",
    "**NOTE:** _Faster R-CNN uses 0.7 default threshold_ as mentioned in the lecture slides. We use a lower threeshold to increase the number of positive matches for sampling — this helps in speeding up training in a resource constrained setting like Google Colab.\n",
    "\n",
    "> _Multi-scale matching_ for different FPN levels — like FCOS, Faster R-CNN also considers only a subset of boxes for each level, based on their size. However the exact matching rule is slightly different. Intuitively, larger boxes are assigned to `p5` and smaller boxes are assigned to `p3`.\n",
    "\n",
    "Anchor boxes with `IoU < 0.3` with ALL GT boxes is assigned background GT box `(-1, -1, -1, -1, -1)`. All other anchors with IoU between `(0.3, 0.6)` are considered \"neutral\" and ignored during training. This matching differs from FCOS, which assigns ALL anchors to either object or background — the \"neutral\" Faster R-CNN anchors cause wasted computation, and removing this redundancy would overly complicate our implementation.\n",
    "\n",
    "We have implemented this matching procedure for you — see `rcnn_match_anchors_to_gt` of `two_stage_detector.py`.\n",
    "Read its documentation to understand its input/output format, it is fine if you do not understand its inner working.\n",
    "You only need to understand that it serves the same purpose as location matching in FCOS — to define GT targets for model predictions during training.\n",
    "\n",
    "**NOTE:** We have a conceptually similar function `rcnn_match_proposals_to_gt` for matching in the second stage of Faster R-CNN. It is conceptually similar to `rcnn_match_anchors_to_gt` but without the multi-scale FPN matching (which introduces complications). You will eventually get to it later in the notebook.\n",
    "\n",
    "This function internally requires IoU computation between all anchors and GT boxes — which you have to implement.\n",
    "Follow the instructions in `two_stage_detector.py` to implement IoU computation, and execute the next cell for a sanity check — you should observe an error of `1e-7` or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 39235,
     "status": "ok",
     "timestamp": 1604336795669,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "fK_USCuaXSzh",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "eda44250-2f2f-4cbd-8e53-aec8d452b800",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import iou\n",
    "\n",
    "\n",
    "boxes1 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n",
    "boxes2 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n",
    "\n",
    "expected_iou = torch.Tensor(\n",
    "    [[1.0, 0.0625, 0.25], [0.0625, 0.0, 0.052631579], [0.0625, 1.0, 0.052631579]]\n",
    ")\n",
    "result_iou = iou(boxes1, boxes2)\n",
    "\n",
    "print(\"Relative error:\", rel_error(expected_iou, result_iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing matched GT boxes\n",
    "\n",
    "Now we apply our anchor matching function and visualize one GT box with a random matched positive anchor box.\n",
    "You may try different images by indexing `train_dataset` below,\n",
    "make sure to try different FPN levels as certain images may not get any matched GT boxes due to their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 40245,
     "status": "ok",
     "timestamp": 1604336796690,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "wPvX4TrgaLD8",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "8ea32253-e09d-4a92-c8b3-b27f816969cd",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from common import get_fpn_location_coords\n",
    "from two_stage_detector import generate_fpn_anchors, rcnn_match_anchors_to_gt\n",
    "\n",
    "\n",
    "# Sanity check: Match anchors of p4 level with GT boxes of first image\n",
    "# in the training dataset.\n",
    "_, image, gt_boxes = train_dataset[0]\n",
    "\n",
    "FPN_LEVEL = \"p4\"\n",
    "FPN_STRIDE = 16\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={FPN_LEVEL: (2, 64, 224 // FPN_STRIDE, 224 // FPN_STRIDE)},\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE}\n",
    ")\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE},\n",
    "    stride_scale=8,  # Default value used by Faster R-CNN\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "\n",
    "matched_boxes_per_fpn_level = rcnn_match_anchors_to_gt(\n",
    "    anchors, gt_boxes, iou_thresholds=(0.3, 0.6)\n",
    ")\n",
    "\n",
    "# Flatten anchors and matched boxes:\n",
    "anchors_p4 = anchors[FPN_LEVEL].view(-1, 4)\n",
    "matched_boxes_p4 = matched_boxes_per_fpn_level[FPN_LEVEL].view(-1, 5)\n",
    "\n",
    "# Visualize one selected anchor and its matched GT box.\n",
    "# NOTE: Run this cell multiple times to see different matched anchors. For car\n",
    "# image, p3/5 will not work because the GT box was already assigned to p4.\n",
    "fg_idxs_p4 = (matched_boxes_p4[:, 4] > 0).nonzero()\n",
    "fg_idx = random.choice(fg_idxs_p4)\n",
    "\n",
    "# Combine both boxes for visualization:\n",
    "dummy_vis_boxes = [anchors_p4[fg_idx][0], matched_boxes_p4[fg_idx][0]]\n",
    "\n",
    "print(\"Unlabeled red box is positive anchor:\")\n",
    "detection_visualizer(\n",
    "    inverse_norm(image),\n",
    "    val_dataset.idx_to_class,\n",
    "    bbox=dummy_vis_boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "XW_Zek3_dgfF",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### GT Targets for box regression\n",
    "\n",
    "Now we work on the third and final component needed to train our RPN — we define transformation functions for box regression deltas.\n",
    "Recall in the first part of the assignment, you implemented two such functions for FCOS (quoting from `one_stage_detector.ipynb`):\n",
    "\n",
    "> 1. `fcos_get_deltas_from_locations`: Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
    "> 2. `fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Here you will implement similar transformation functions for R-CNN, albeit with a different transformation logic than FCOS. You will find these transforms in [Lecture 13, slides 68-71](https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture13.pdf), follow these and implement two functions in `two_stage_detector.py`:\n",
    "\n",
    "1. `rcnn_get_deltas_from_anchors`: Accepts anchor boxes and GT boxes, and returns deltas. Required for training supervision.\n",
    "2. `rcnn_apply_deltas_to_anchors`: Accepts predicted deltas and anchor boxes, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 40233,
     "status": "ok",
     "timestamp": 1604336796690,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "MX2JCaOf0768",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "b550a5c7-75af-4f85-da5a-c1ff684de6af",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import rcnn_get_deltas_from_anchors, rcnn_apply_deltas_to_anchors\n",
    "\n",
    "# Three hard-coded anchor boxes and GT boxes that have a fairly high overlap.\n",
    "# Add a dummy class ID = 1 indicating foreground\n",
    "input_anchors = torch.Tensor(\n",
    "    [[20, 40, 80, 90], [10, 10, 50, 50], [120, 100, 200, 200]]\n",
    ")\n",
    "input_boxes = torch.Tensor(\n",
    "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
    ")\n",
    "\n",
    "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
    "# and applying them back to anchors should give us the same boxes.\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchors, input_boxes)\n",
    "output_boxes = rcnn_apply_deltas_to_anchors(_deltas, input_anchors)\n",
    "\n",
    "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
    "\n",
    "# Another check: deltas for GT class label = -1 should be -1.\n",
    "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
    "input_anchor = torch.Tensor([[100, 100, 200, 200]])\n",
    "\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchor, background_box)\n",
    "output_box = rcnn_apply_deltas_to_anchors(_deltas, input_anchor)\n",
    "\n",
    "print(\"Background deltas should be all -1e8  :\", _deltas)\n",
    "print(\"Output box should be -1e8 or lower    :\", output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "dlO2IUCnt4zu",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "With all predictions assigned with GT targets, we will proceed to compute losses for training the RPN.\n",
    "Recall that you used [Focal Loss](https://arxiv.org/abs/1708.02002) for classification and L1 loss for box regression in FCOS.\n",
    "Here, you will use L1 loss for box regression, similar to FCOS.\n",
    "\n",
    "**Objectness classification loss:** Focal Loss was proposed in RetinaNet (2017) to deal with heavy class imbalance caused by \"background\". Faster R-CNN predates this paper — it dealt with class imbalance by randomly sampling roughly equal amount of foreground-background anchors to train RPN. We have implemented a very simple sampling function for you in `sample_rpn_training` function of `two_stage_detector.py` — you may directly use it while you piece all these components (coming up next).\n",
    "\n",
    "**Total loss** is the sum of both loss components _per sampled anchor_, averaged by total number of foreground + background anchors.\n",
    "\n",
    "Execute the next cell to quickly recap their usage — you have already seen these in the first part of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 40224,
     "status": "ok",
     "timestamp": 1604336796691,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "2eSleGX9yTeo",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "ccd06602-78a2-43fb-ee01-a5431da59393",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Sanity check: dummy predictions from model - box regression deltas and\n",
    "# objectness logits for two anchors.\n",
    "# shape: (batch_size, HWA, 4 or 1)\n",
    "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
    "dummy_pred_obj_logits = torch.randn(1, 2, 1)\n",
    "\n",
    "# Dummy deltas and objectness targets. Let the second box be background.\n",
    "# Dummy GT boxes (matched with both anchors).\n",
    "dummy_gt_deltas = torch.randn_like(dummy_pred_boxreg_deltas)\n",
    "dummy_gt_deltas[:, 1, :] = -1e8\n",
    "\n",
    "# Background objectness targets should be 0 (not -1), and foreground\n",
    "# should be 1. Neutral anchors will not occur here due to sampling.\n",
    "dummy_gt_objectness = torch.Tensor([1, 0])\n",
    "\n",
    "# Note that loss is not multiplied with 0.25 here:\n",
    "loss_box = F.l1_loss(\n",
    "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# No loss for background anchors:\n",
    "loss_box[dummy_gt_deltas == -1e8] *= 0.0\n",
    "print(\"Box regression loss (L1):\", loss_box)\n",
    "\n",
    "# Now calculate centerness loss.\n",
    "loss_obj = F.binary_cross_entropy_with_logits(\n",
    "    dummy_pred_obj_logits.view(-1), dummy_gt_objectness, reduction=\"none\"\n",
    ")\n",
    "print(\"Objectness loss (BCE):\", loss_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "AiPfXUHPupDE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Putting it all together: RPN module\n",
    "\n",
    "Now you will put together all the things you have implemented into the `RPN` class in `two_stage_detector.py`.\n",
    "Implement `forward` and `predict_proposals` functions of this module — you have already done most of the heavy lifting, you simply need to call all the functions in a correct way!\n",
    "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
    "\n",
    "**TIP:** It may help if you draw analogies between the implementation logic in this module vs FCOS (`RPN.predict_proposals()` -> `FCOS.inference()`).\n",
    "\n",
    "## Overfit small data\n",
    "\n",
    "In Faster R-CNN, the RPN is trained jointly with the second-stage network.\n",
    "However, to test our RPN implementation, we will first train just the RPN — this is basically a class-agnostic FCOS without centerness.\n",
    "We will use the `train_detector` function that we used for training FCOS.\n",
    "You can read its implementation in `a4_helper.py`. \n",
    "\n",
    "The loss should generally do down, however the forward pass here is a bit slower than FCOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 61721,
     "status": "ok",
     "timestamp": 1604336818203,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "YTObddiog9wJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "aa4bcb05-2b38-4006-8f89-ca4a9af3835d",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from a4_helper import train_detector\n",
    "from common import DetectorBackboneWithFPN\n",
    "from two_stage_detector import RPN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Take equally spaced examples from training dataset to make a subset.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create a wrapper module to contain backbone + RPN:\n",
    "class FirstStage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = DetectorBackboneWithFPN(out_channels=32)\n",
    "        self.rpn = RPN(\n",
    "            fpn_channels=32,\n",
    "            stem_channels=[32],\n",
    "            batch_size_per_image=64,\n",
    "            anchor_stride_scale=8,\n",
    "            anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
    "            anchor_iou_thresholds=(0.3, 0.7),\n",
    "            nms_thresh=0.7,\n",
    "            pre_nms_topk=250,\n",
    "            post_nms_topk=50,\n",
    "        )\n",
    "\n",
    "    def forward(self, images, gt_boxes=None):\n",
    "        feats_per_fpn_level = self.backbone(images)\n",
    "        return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n",
    "\n",
    "\n",
    "first_stage = FirstStage().to(DEVICE)\n",
    "\n",
    "train_detector(\n",
    "    first_stage,\n",
    "    small_train_loader,\n",
    "    learning_rate=0.02,\n",
    "    max_iters=100,\n",
    "    log_period=10,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "jKjv6JLMRj7s",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Faster R-CNN\n",
    "\n",
    "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
    "\n",
    "Given a set of proposal boxes from RPN (per FPN level, per image),\n",
    "we warp each region from the correspondng map to a fixed size 3x3 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf). Note that Faster R-CNN uses 7x7 map.\n",
    "We will use the `roi_align` function from `torchvision`. Usage see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align\n",
    "\n",
    "For simplicity and computational constraints of Google Colab,\n",
    "our two-stage detector here differs from a full Faster R-CNN system in a few aspects:\n",
    "\n",
    "1. In a full implementation, the second stage of the network would predict a box deltas to further refine RPN proposals. However we omit this for simplicity - we keep RPN proposal boxes as final predicted boxes. Your model will definitely perform better with a box predictor.\n",
    "\n",
    "2. Second stage uses a `(C+1)` softmax classifier to classify proposal boxes. We instead use a simple sigmoid focal loss like we did FCOS. We opted for this design choice for you to re-use a lot of concepts that you already implemented in FCOS - choosing one loss over other matters less overall.\n",
    "\n",
    "\n",
    "**NOTE: YOUR IMPLEMENTATION EXERCISE.**\n",
    "\n",
    "Read `FasterRCNN` class documentation and code to understand how everything is pieced together.\n",
    "By now you have already implemented the core components of a typical object detection system - you have dealt with anchor boxes or locations (FCOS), matched them with GT boxes, supervised model with your matching, and wrote inference utilities like NMS.\n",
    "Great work!\n",
    "\n",
    "Beyond these, the second stage of Faster R-CNN doesn't add anything that is conceptually new — hence your implementation exercise is fairly lightweight.\n",
    "We have implemented most of this module for you. We left out a few 3-4 line TODO blocks, only because if we wrote them, they would given away the solution for prior exercises (RPN and FCOS).\n",
    "Moreover, empty code blocks will encourage you to carefully read the remaining portions for making everything work.\n",
    "Feel free to refer/re-use your own implementation from the first part of the assignment for filling these blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "RFZ49wox4MYn",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "After adding your implementation, overfit the model on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 87583,
     "status": "ok",
     "timestamp": 1604336844074,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "WbxeAJq0zc3F",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "6a9c8d88-29c7-4a83-db92-8e75b2522d3d",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import FasterRCNN\n",
    "\n",
    "\n",
    "backbone = DetectorBackboneWithFPN(out_channels=32)\n",
    "rpn = RPN(fpn_channels=32, stem_channels=[32], batch_size_per_image=64)\n",
    "\n",
    "faster_rcnn = FasterRCNN(backbone, rpn, num_classes=20, roi_size=(3, 3))\n",
    "\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    learning_rate=0.02,\n",
    "    max_iters=500,\n",
    "    log_period=10,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "_SWA1DbG47ln",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now, follow the instructions in `FasterRCNN.inference` to implement inference, similar to `FCOS.inference`.\n",
    "\n",
    "Visualize the output from the trained model on a few eval images by running the code below, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224,
     "output_embedded_package_id": "11365lX4HCQuMWjqvDNZLW_bbkmjBYgPa"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 92245,
     "status": "ok",
     "timestamp": 1604336848739,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "gp_Hmt-Km5bl",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "b2fe0c8e-fb0e-4151-8021-ca7637c95b2a",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.5,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "sr7wNngy4oZf",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Train a net\n",
    "\n",
    "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data.\n",
    "We will train for 2000 iterations; this should take about 30 minutes on a K80 GPU.\n",
    "For initial debugging, you may want to train for lesser durations (say 100 iterations).\n",
    "\n",
    "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
    "\n",
    "(Optional) If you train the model longer (e.g., 100 epochs), you should see a better mAP. But make sure you revert the code back for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 1245035,
     "status": "ok",
     "timestamp": 1604338388407,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "X1k1rx1f4sTE",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "53016a93-7d20-4bd1-e53c-5e2ea819af05",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# Slightly larger detector than in above cell.\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
    "rpn = RPN(fpn_channels=64, stem_channels=[64, 64], batch_size_per_image=64)\n",
    "faster_rcnn = FasterRCNN(backbone, rpn, num_classes=20, roi_size=(3, 3))\n",
    "\n",
    "\n",
    "# Train for shorter duration than FCOS because forward pass is slower:\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    train_loader,\n",
    "    learning_rate=1e-3,\n",
    "    max_iters=2000,\n",
    "    log_period=100,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# After you've trained your model, save the weights for submission.\n",
    "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"rcnn_detector.pt\")\n",
    "torch.save(faster_rcnn.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "KhWZT-ztEaqm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1ichXEDYlWPmfoeTM9K6emTm7xyEiop4b"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 4557,
     "status": "ok",
     "timestamp": 1604338499939,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "J7ArGiLTnHta",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "ba366134-e564-40db-c4ec-f6b18c0abb08",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from a4_helper import inference_with_detector\n",
    "from two_stage_detector import RPN, FasterRCNN\n",
    "\n",
    "\n",
    "# Re-initialize so this cell is independent from prior cells.\n",
    "# Slightly larger detector than in above cell.\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
    "rpn = RPN(fpn_channels=64, stem_channels=[64, 64], batch_size_per_image=64)\n",
    "faster_rcnn = FasterRCNN(backbone, rpn, num_classes=20, roi_size=(3, 3))\n",
    "faster_rcnn.to(device=DEVICE)\n",
    "\n",
    "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"rcnn_detector.pt\")\n",
    "faster_rcnn.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Prepare a small val daataset for inference:\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    val_dataset,\n",
    "    torch.linspace(0, len(val_dataset) - 1, steps=10).long()\n",
    ")\n",
    "small_val_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.5,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "ETU6ev7aydIY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate your Faster R-CNN like FCOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "deletable": true,
    "executionInfo": {
     "elapsed": 39181,
     "status": "ok",
     "timestamp": 1604338539394,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "FvDb7uwqyhAK",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "f39788ae-e90a-418c-e625-bd52d956703c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.4,\n",
    "    nms_thresh=0.6,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    "    output_dir=\"mAP/input\",\n",
    ")\n",
    "!cd mAP && python main.py\n",
    "\n",
    "# This script outputs an image containing per-class AP. Display it here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./mAP/output/mAP.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "4u69TlnhucrR",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Submit Your Work\n",
    "After completing both notebooks for this assignment (`one_stage_detector.ipynb` and this notebook, `two_stage_detector.ipynb`), run the following cell to create a `.zip` file for you to download and turn in. \n",
    "\n",
    "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "executionInfo": {
     "elapsed": 220660,
     "status": "aborted",
     "timestamp": 1604336977202,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "e6vziUpSuqLY",
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from eecs598.submit import make_a4_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "make_a4_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xVQPPDVchAGQ",
    "AOIazbsrFEVc",
    "RFZ49wox4MYn"
   ],
   "name": "two_stage_detector_faster_rcnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
